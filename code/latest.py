# -*- coding: utf-8 -*-
"""20newsgroup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14PztWDQzJr7ViaJ3WQsZe7rTzJ1xCtjA
"""


import os
import tarfile
import matplotlib.pyplot as plt
import pandas as pd
import torch
from pathlib import Path
from NeuralNMF import train
from sklearn.feature_extraction.text import TfidfVectorizer
from NeuralNMF import Neural_NMF
import pickle



train_data_dir = Path("/home/Tanishk/20news-bydate/20news-bydate-train")
test_data_dir = Path("/home/Tanishk/20news-bydate/20news-bydate-test")

# Function to load data
def load_20newsgroups(data_dir):
    data = []
    target = []

    for newsgroup in os.listdir(data_dir):
        newsgroup_path = os.path.join(data_dir, newsgroup)

        if os.path.isdir(newsgroup_path):
            for filename in os.listdir(newsgroup_path):
                file_path = os.path.join(newsgroup_path, filename)

                if os.path.isfile(file_path):
                    with open(file_path, 'r', encoding='latin1') as file:
                        content = file.read()
                        data.append(content)
                        target.append(newsgroup)

    return pd.DataFrame({'text': data, 'newsgroup': target})

train_df = load_20newsgroups(train_data_dir)
test_df = load_20newsgroups(test_data_dir)


tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english', max_features=5000)
tfidf = tfidf_vectorizer.fit_transform(train_df['text'])

X = torch.tensor(tfidf.toarray(), dtype=torch.float64)

m = X.shape[0]
k2 = 1000
k3 = 400
k4 = 20

net = Neural_NMF([m, k2, k3, k4])


#def create_batches(X1, batch_size):
#    num_samples = 11300
#   indices = torch.randperm(num_samples)
#    return [X1[indices[i:i + batch_size]] for i in range(0, num_samples, batch_size)]

# Create batches
#batch_size = 100  # Adjust batch size as needed
#batches = create_batches(X1, batch_size)

history = []

# Function to train on batches

history = train(net, X, epoch=10, lr=500, supervised=False)


with open("history.pkl", "wb") as file:
    pickle.dump(history, file)
